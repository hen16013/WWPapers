{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import visibility_of_element_located\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from difflib import context_diff\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Scriptures from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptures = pd.read_csv(\"C:/Users/LeRoy12358/Documents/School/W23/499 DS - Senior Project/WWPapers/raw data/lds-scriptures.csv\")\n",
    "\n",
    "# scriptures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping one page and extracting list of directions for other pages to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()  # start a web browser\n",
    "browser.get(\"https://scriptures.byu.edu/#:t3d092:p573\")  # navigate to URL\n",
    "# wait for page to load\n",
    "# by waiting for <h1> element to appear on the page\n",
    "title = (\n",
    "    WebDriverWait(driver=browser, timeout=10)\n",
    "    .until(visibility_of_element_located((By.ID, \"jContent\")))\n",
    "    .text\n",
    ")\n",
    "# retrieve fully rendered HTML content\n",
    "content = browser.page_source\n",
    "browser.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we then could parse it with beautifulsoup\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "# print(soup.find(\"jContent\").text)\n",
    "# soup\n",
    "\n",
    "# soup.div['id']\n",
    "\n",
    "# print(soup.prettify())\n",
    "# content\n",
    "# soup.div.div.div\n",
    "\n",
    "soup.find(\"div\", {\"id\": \"jContent\"})\n",
    "tmp = soup.find('div', {'class': 'discourseBody'}).text\n",
    "\n",
    "# tmp.replace(\"\\xa0\", \"\")\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find('ul', {'class': 'talksblock'})\n",
    "\n",
    "linkList = links.find_all('a', {'class': 'refcounter'})\n",
    "\n",
    "link1 = links.find('a', {'class': 'refcounter'})['onclick']\n",
    "\n",
    "# type(link1)\n",
    "link1.replace(\"getTalk('\", \"\").replace(\"');\", \"\")\n",
    "# for item in linkList:\n",
    "#     # item.replace(\"getTalk('\", \"\").replace(\"');\", \"\")\n",
    "#     # print(item)\n",
    "#     pass\n",
    "# linkList.replace(\"getTalk('\", \"\").replace(\"');\", \"\")\n",
    "\n",
    "linkList2 = []\n",
    "\n",
    "for link in soup.find_all('a', {'class': 'refcounter'}):\n",
    "    linkList2.append(\n",
    "        np.base_repr(int(\n",
    "        link.get('onclick').replace(\"getTalk('\", \"\").replace(\"');\", \"\")\n",
    "        ), 16\n",
    "        ).lower()\n",
    "        )\n",
    "    \n",
    "# http://example.com/elsie\n",
    "# http://example.com/lacie\n",
    "# http://example.com/tillie\n",
    "\n",
    "# linkList2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping all the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will likely take quite a while to run....\n",
    "\n",
    "# breaker = False\n",
    "browser = webdriver.Chrome()  # start a web browser\n",
    "pot = []\n",
    "rawPot = []\n",
    "\n",
    "for i in range(0, len(linkList2)):\n",
    "    url = linkList2[i]\n",
    "    print(url, end = \", \")\n",
    "    \n",
    "    try:\n",
    "\n",
    "\n",
    "        browser.get(\"https://scriptures.byu.edu/#:t\" + url + \":p573\")  # navigate to URL\n",
    "        # wait for page to load by waiting for <h1> element to appear on the page\n",
    "        title = (\n",
    "            WebDriverWait(driver=browser, timeout=10)\n",
    "            .until(visibility_of_element_located((By.ID, \"jContent\")))\n",
    "            .text\n",
    "        )\n",
    "        # retrieve fully rendered HTML content\n",
    "        content = browser.page_source\n",
    "\n",
    "    except:\n",
    "        print(\"Something broke, autofixing....\", end = \" \")\n",
    "        browser.close()\n",
    "        browser = webdriver.Chrome()\n",
    "        i -= 1\n",
    "        # continue\n",
    "    else:\n",
    "        # we then could parse it with beautifulsoup\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        rawPot.append(soup.find('div', {'class': 'discourseBody'}))\n",
    "        pot.append(soup.find('div', {'class': 'discourseBody'}).text)\n",
    "        # pot.append(soup.find_all('a', {'href': 'discourseBody'}))\n",
    "        time.sleep(.11)\n",
    "    \n",
    "browser.close()\n",
    "    # if(breaker):\n",
    "    #     break\n",
    "    # breaker = True\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the parsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extracting list of references from a single talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scriptures.verse_short_title = scriptures.verse_short_title.str.replace(\"JS—H\", \"JS-H\")\n",
    "# rawPot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference = \"Luke 8:1 (JST)\".replace(\"—\", \"-\").replace(\" (JST)\", \"\")\n",
    "\n",
    "# reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting a list of references \n",
    "def extractReferences(soup):\n",
    "    citationList = soup.find_all('span', {'class': 'citation'})\n",
    "\n",
    "    soupReferenceList = []\n",
    "    for citation in citationList:\n",
    "        soupReferenceList.append(\n",
    "        citation.find_all('a', {'href': 'javascript:void(0)'})[1].text.replace(\"—\", \"-\").replace(\" (JST)\", \"\")\n",
    "        )\n",
    "        # print(citation.find('span', {'class': 'citation'}))\n",
    "    \n",
    "    return soupReferenceList\n",
    "\n",
    "# creating a variable containing only the first speech\n",
    "tmp = rawPot[0]\n",
    "referenceList = extractReferences(tmp)\n",
    "\n",
    "print(referenceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myRef = \"JS—H 1:68\"\n",
    "# scriptureFromReference(myRef)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finds references in the speech and extracts preceding N characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a scripture reference as held in referenceList and return the scripture contents\n",
    "\n",
    "def scriptureFromReference(reference):\n",
    "    # print(reference)\n",
    "    try: # Also not optimal, but most of them work just fine and it's more important that SOME work right now than that ALL work.\n",
    "        # Not optimal, but I don't have time to debug the interaction between - and , in determining what verses should be chosen\n",
    "        # TODO: Make it actually work\n",
    "        if \",\" in reference:\n",
    "            return \"\"\n",
    "        if \"-\" in reference: # Isa. 40:1-3\n",
    "            bookCh, verses = reference.split(\":\") #[Isa. 40, 1-3]\n",
    "            if \"-\" in verses: #JS-H 1, 1-3 \n",
    "                verseStart, verseEnd = verses.split(\"-\") # Isa. 40, 1, \n",
    "            else: # JS-H 1, 3\n",
    "                verseStart = verseEnd = verses\n",
    "            specificScripture = \"\"\n",
    "            for i in range(int(verseStart), int(verseEnd) + 1):\n",
    "                fullReference = bookCh + \":\" + str(i)\n",
    "                specificScripture = specificScripture + scriptures.query(\n",
    "                    \"verse_short_title == @fullReference\").scripture_text.iloc[0]\n",
    "            return specificScripture\n",
    "        else:\n",
    "            specificScripture = scriptures.query(\"verse_short_title == @reference\").scripture_text.iloc[0]\n",
    "        return(specificScripture)\n",
    "    except:\n",
    "        print(\"Failed to read: \", end = \"\")\n",
    "        print(reference)\n",
    "        return \"\"\n",
    "\n",
    "# result = scriptureFromReference(referenceList[3])#\"Heb. 11:37\")\n",
    "# scriptureFromReference(referenceList[2])\n",
    "# scriptureFromReference(referenceList[9])\n",
    "# result\n",
    "\n",
    "# takes an iterable list of scripture references and returns an iterable list of the contents of each scripture\n",
    "def scriptureFromReferenceVectorized(refList):#, resultList):\n",
    "    resultList = []\n",
    "    # resultList = pd.Series(resultList, dtype = object)\n",
    "    # print(type(resultList))\n",
    "    for ref in refList:\n",
    "        # resultList = pd.concat([resultList, pd.Series(scriptureFromReference(ref), dtype = object)])\n",
    "        # print(resultList)\n",
    "        resultList.append(scriptureFromReference(ref))\n",
    "    return resultList\n",
    "\n",
    "# resultList = []\n",
    "scripList = scriptureFromReferenceVectorized(referenceList)\n",
    "# scriptureFromReferenceVectorized([referenceList[0], referenceList[1], referenceList[2]])\n",
    "# type(referenceList)\n",
    "# referenceList.str.len()\n",
    "\n",
    "# scriptureFromReference(\"Isa. 40:1-3\")\n",
    "# scripList\n",
    "# referenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JS—H 1:68-72\n",
    "# \"—\" in \"-\"\n",
    "# \"-\" in \"—\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultList = []\n",
    "# # resultList = pd.Series(resultList, dtype = object)\n",
    "# # print(type(resultList))\n",
    "# for ref in referenceList:\n",
    "#     # resultList = pd.concat([resultList, pd.Series(scriptureFromReference(ref), dtype = object)])\n",
    "#     resultList.append(scriptureFromReference(ref))\n",
    "# # print(scriptureFromReference(ref))\n",
    "# # print(scriptureFromReferenceVectorized(referenceList))\n",
    "# print(resultList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTmp = pd.Series(rawPot[0].text.replace(\"\\xa0\", \"\").replace(\"—\", \"-\"))\n",
    "\n",
    "surrCharSets = pd.DataFrame(columns = [\"charSet\", \"ref\"])\n",
    "# surrCharSets.columns = [\"charSet\", \"ref\"]\n",
    "\n",
    "for ref in referenceList:\n",
    "    # this bit to suppress warnings about regex boolean values\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        singleCharPair = myTmp.str.extract(\"(.{100})(\" + ref + \")\")\n",
    "        singleCharPair.columns = [\"charSet\", \"ref\"]\n",
    "        surrCharSets = pd.concat([surrCharSets, singleCharPair], ignore_index= True)\n",
    "        # remove the just found instance of the reference and only that instance (n = 1)\n",
    "        myTmp = myTmp.str.replace(ref, \"\", n = 1)\n",
    "    # print(ref, end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(referenceList)\n",
    "# type(list(surrCharSets['ref']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the references looks up the actual scripture text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultList2 = []\n",
    "# resultList2 = pd.Series(resultList2)\n",
    "\n",
    "# resultList2 = \n",
    "# scriptureFromReferenceVectorized(referenceList, resultList2)\n",
    "\n",
    "# print(resultList2)\n",
    "\n",
    "# surrCharSets['scriptureContents'] = resultList2\n",
    "surrCharSets['scriptureContents'] = scriptureFromReferenceVectorized(list(surrCharSets['ref']))\n",
    "# surrCharSets['scriptureContents'] = scriptureFromReferenceVectorized(referenceList)\n",
    "\n",
    "# print(referenceList)\n",
    "# list(surrCharSets.ref)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ref in surrCharSets.ref:\n",
    "#     print(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 50)\n",
    "# # list(surrCharSets.ref)\n",
    "# surrCharSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(redundantReferenceList)\n",
    "# print(list(surrCharSets.ref))\n",
    "# print(referenceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['1 Ne. 13:12', 'D&C 135:3', 'D&C 136:39', 'Heb. 11:37', 'Rev. 6:9', '2 Pet. 1:21', 'Gen. 2:7', 'Isa. 31:1', 'Prov. 14:34', 'Gal. 6:7', 'Matt. 7:2', 'Isa. 40:1-3', 'Jer. 31:1-14', 'Ezek. 20:33-44', 'D&C 5:14', 'D&C 33:5', 'D&C 109:73', 'Eph. 2:20', '1 Pet. 2:6', 'Dan. 2:44', 'Dan. 2:34-35', 'D&C 65:2', 'Dan. 2:44', 'Isa. 2:2-3', 'Rev. 1:9', 'Rev. 14:6-7', 'Rev. 1:9', 'Rev. 6:9', 'D&C 13:1', 'D&C 27:8', \n",
    "# nan,\n",
    "#  'D&C 27:12', 'John 15:19', 'Isa. 60:22', 'Rom. 1:16', 'D&C 84:33', 'D&C 84:38-39', 'D&C 84:41', 'Micah 3:11', 'Acts 19:27', '1 Cor. 12:28', 'Eph. 2:20', 'Isa. 14:12', 'D&C 76:26', 'Jer. 25:31', 'Luke 21:24', 'Rev. 5:9', 'Rev. 8:6', 'Rev. 16:1', 'Rev. 21:9', 'Rev. 16:19', 'Isa. 34:5-6', 'D&C 1:36', 'Heb. 11:3', 'Heb. 11:7', 'Gen. 6:3', 'Moses 8:17', 'Heb. 11:8', 'Deut. 20:3', 'D&C 103:19', 'D&C 88:2', 'D&C 98:2', 'D&C 58:22', 'Hosea 8:7', '3 Ne. 27:27', 'Luke 17:33', 'Eph. 4:10', 'D&C 88:6', 'D&C 122:8', 'D&C 122:8', 'Rev. 2:10']\n",
    "# ['1 Ne. 13:12', 'D&C 135:3', 'D&C 136:39', 'Heb. 11:37', 'Rev. 6:9', '2 Pet. 1:21', 'Gen. 2:7', 'Isa. 31:1', 'Prov. 14:34', 'Gal. 6:7', 'Matt. 7:2', 'Isa. 40:1-3', 'Jer. 31:1-14', 'Ezek. 20:33-44', 'D&C 5:14', 'D&C 33:5', 'D&C 109:73', 'Eph. 2:20', '1 Pet. 2:6', 'Dan. 2:44', 'Dan. 2:34-35', 'D&C 65:2', 'Dan. 2:44', 'Isa. 2:2-3', 'Rev. 1:9', 'Rev. 14:6-7', 'Rev. 1:9', 'Rev. 6:9', 'D&C 13:1', 'D&C 27:8', \n",
    "# 'JS-H 1:68-72',\n",
    "#  'D&C 27:12', 'John 15:19', 'Isa. 60:22', 'Rom. 1:16', 'D&C 84:33', 'D&C 84:38-39', 'D&C 84:41', 'Micah 3:11', 'Acts 19:27', '1 Cor. 12:28', 'Eph. 2:20', 'Isa. 14:12', 'D&C 76:26', 'Jer. 25:31', 'Luke 21:24', 'Rev. 5:9', 'Rev. 8:6', 'Rev. 16:1', 'Rev. 21:9', 'Rev. 16:19', 'Isa. 34:5-6', 'D&C 1:36', 'Heb. 11:3', 'Heb. 11:7', 'Gen. 6:3', 'Moses 8:17', 'Heb. 11:8', 'Deut. 20:3', 'D&C 103:19', 'D&C 88:2', 'D&C 98:2', 'D&C 58:22', 'Hosea 8:7', '3 Ne. 27:27', 'Luke 17:33', 'Eph. 4:10', 'D&C 88:6', 'D&C 122:8', 'D&C 122:8', 'Rev. 2:10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # surrCharSets['refContents'] = scriptureFromReferenceVectorized(surrCharSets.ref)\n",
    "# # surrCharSets['refContents'] = scriptureFromReferenceVectorized(surrCharSets.ref)\n",
    "# # surrCharSets = surrCharSets.assign(refContents = lambda x: scriptureFromReference(x.ref))\n",
    "\n",
    "# # map(fun, iter)\n",
    "\n",
    "# list(\n",
    "#     map(\n",
    "#     scriptureFromReference, list(surrCharSets.ref)\n",
    "#     )\n",
    "#     )\n",
    "\n",
    "# redundantReferenceList = []\n",
    "# for item in surrCharSets.ref:\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter(\"ignore\")\n",
    "#         redundantReferenceList.append(item)\n",
    "\n",
    "# list(\n",
    "#     map(\n",
    "#     scriptureFromReference, redundantReferenceList#list(surrCharSets.ref)\n",
    "#     )\n",
    "#     )\n",
    "# # surrCharSets['refContents'] = 3\n",
    "\n",
    "# # surrCharSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singleRefCharPairSeries = myTmp.str.extract(\"(.{100})(\" + ref + \")\")\n",
    "# singleRefCharPairDf = pd.DataFrame(data = {\"charSet\": singleRefCharPairSeries[0], \"ref\": singleRefCharPairSeries[1]})\n",
    "# # type(singleRefCharPairDf)\n",
    "# singleRefCharPairSeries.columns = [\"charSet\", \"ref\"]\n",
    "# pd.concat([singleRefCharPairSeries, singleRefCharPairDf])\n",
    "# # singleRefCharPairDf\n",
    "\n",
    "surrCharSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Temporarily Removed\n",
    "\n",
    "# surrCharSets[\"charSetPunctRemoved\"] = surrCharSets['charSet']\n",
    "\n",
    "# surrCharSets.charSetPunctRemoved = surrCharSets.charSetPunctRemoved.str.replace(\",|\\\\.|!|\\\\?\", \"\")\n",
    " \n",
    "surrCharSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatches(str1, str2, BLOCK_MIN_LENGTH = 8):\n",
    "    sm = SequenceMatcher(lambda x: x in \"\", str1, str2)\n",
    "    finalMatches = []\n",
    "\n",
    "    # print(0)\n",
    "\n",
    "    matchesPosns = sm.get_matching_blocks()\n",
    "\n",
    "    # print(1)\n",
    "\n",
    "    # put together a list of matches longer than only a couple characters\n",
    "\n",
    "    # print(matchesPosns)\n",
    "\n",
    "    for matchPos in matchesPosns:\n",
    "        # print(matchPos)\n",
    "        matchLength = matchPos[2]\n",
    "        # print(2)\n",
    "        if matchLength < BLOCK_MIN_LENGTH:\n",
    "            continue\n",
    "        # print(str1[matchPos[0]:matchPos[0] + matchPos[2]])\n",
    "        # print(3)\n",
    "        finalMatches.append(matchPos)\n",
    "    # print(\"Final Matches:\", finalMatches)\n",
    "    try:\n",
    "        return [str1[finalMatches[0][0]:finalMatches[-1][0] + finalMatches[-1][2]],\n",
    "                str2[finalMatches[0][1]:finalMatches[-1][1] + finalMatches[-1][2]]]\n",
    "    except:\n",
    "        return\n",
    "\n",
    "\n",
    "surrCharSets[\"wilfordMatch\"] = surrCharSets[\"scriptureMatch\"] = \"\"\n",
    "    \n",
    "# woodruffMatches = []\n",
    "# scriptureMatches = []\n",
    "\n",
    "for index, row in surrCharSets.iterrows():\n",
    "# for index, row in surrCharSets.sample(n=2).iterrows():\n",
    "    # if index > 0:\n",
    "    #     break\n",
    "    # print(row.charSet)\n",
    "    # print(row.scriptureContents, end = '\\n')\n",
    "    matches = findMatches(row.charSet, row.scriptureContents, 10)\n",
    "    # print(matches, end = \"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        if(len(matches[0]) > 15 and len(matches[1]) > 15):\n",
    "\n",
    "            row[\"wilfordMatch\"], row[\"scriptureMatch\"] = matches\n",
    "        else:\n",
    "            row[\"wilfordMatch\"] = row[\"scriptureMatch\"] = \"\"\n",
    "        \n",
    "    except:\n",
    "        row[\"wilfordMatch\"] = row[\"scriptureMatch\"] = \"\"\n",
    "        pass\n",
    "    # print(type(matches))\n",
    "    # print(\"Other Try: \", end = \"\")\n",
    "    # sm = SequenceMatcher(lambda x: x in \"\", row.charSetPunctRemoved, row.scriptureContents)\n",
    "    # print(sm.find_longest_match(), end = \"\\n\\n\")\n",
    "\n",
    "    # break\n",
    "    # print(myStr1)\n",
    "    # print(myStr2)\n",
    "\n",
    "    # print(woodruffMatch, scriptureMatch)\n",
    "    # print(row.charSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrCharSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need more training data / specific checks for this findMatches function\n",
    "\n",
    "myStr1 = \"He has left on record through the medium of holy men who wrote and spoke as they were moved upon by the Holy Ghost\"\n",
    "myStr2 = \"For the prophecy came not in old time by the will of man: but holy men of God spake as they were moved by the Holy Ghost.\"\n",
    "\n",
    "findMatches(myStr1, myStr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myStr1 = \n",
    "# myStr2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### String Matching Attempt using vectorization of words\n",
    "Computation-intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pip install -U sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = [\"I'm happy\", \"I'm full of happiness\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Compute embedding for both lists\n",
    "# Note from E: interesting - encodings must be computed individually....\n",
    "embedding_1= model.encode(sentences[0], convert_to_tensor=True) \n",
    "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "## tensor([[0.6003]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO for the upgraded string matching function:\n",
    "# convert each string into a list of individual words\n",
    "# --- ACTUALLY DON'T - because the encoding functions are for the full strings\n",
    "# compute similarity score for the sentence pair (one score - to each other)\n",
    "# save score and both 'current best phrases' (perhaps remember top 5 values and pairs?)\n",
    "# repetitively cut off words from the start/end of first/second sentence \n",
    "# --- (alternating? - NOT ALTERNATING - NEED A CLEAR SIGNAL OF \"DEAD END\")\n",
    "# track recent sim scores and when sim score drops for too many iterations in a row, revert to \"best\"\n",
    "# --- plus a couple words? but def less than previous 'max'?\n",
    "# --- comparing how fast match rate rises/drops?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WWPapers work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrCharSets.query('charSet == \"apple\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surrCharSets\n",
    "\n",
    "refMonsterTable = surrCharSets.query('charSet == \"apple\"')\n",
    "\n",
    "secondPass = False\n",
    "passNum = 0\n",
    "\n",
    "for soupServing in rawPot:\n",
    "    #tmp = soup\n",
    "    referenceListSoup = extractReferences(soupServing)\n",
    "\n",
    "    # print(referenceList)\n",
    "    # print(\"One\")\n",
    "\n",
    "    myTmpServing = pd.Series(\n",
    "        soupServing.text.replace(\"\\xa0\", \"\").replace(\"—\", \"-\"))\n",
    "\n",
    "    surrCharSetsTemp = pd.DataFrame(columns = [\"charSet\", \"ref\"])\n",
    "    # surrCharSets.columns = [\"charSet\", \"ref\"]\n",
    "    # print(surrCharSetsTemp)\n",
    "\n",
    "    for ref in referenceListSoup:\n",
    "        # this bit to suppress warnings about regex boolean values\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            # print(ref)\n",
    "            singleCharPairServing = myTmpServing.str.extract(\"(.{100})(\" + ref + \")\")\n",
    "            # print(singleCharPairServing)\n",
    "            singleCharPairServing.columns = [\"charSet\", \"ref\"]\n",
    "            surrCharSetsTemp = pd.concat([surrCharSetsTemp, singleCharPairServing], ignore_index= True)\n",
    "            # remove the just found instance of the reference and only that instance (n = 1)\n",
    "            myTmpServing = myTmpServing.str.replace(ref, \"\", n = 1)\n",
    "        # print(ref, end = \"\")\n",
    "        # print(surrCharSetsTemp)\n",
    "        surrCharSetsTemp['scriptureContents'] = scriptureFromReferenceVectorized(list(surrCharSetsTemp['ref']))\n",
    "\n",
    "    surrCharSetsTemp[\"wilfordMatch\"] = surrCharSetsTemp[\"scriptureMatch\"] = \"\"\n",
    "\n",
    "    for index, row in surrCharSetsTemp.iterrows():\n",
    "    # for index, row in surrCharSets.sample(n=2).iterrows():\n",
    "        # if index > 0:\n",
    "        #     break\n",
    "        # print(row.charSet)\n",
    "        # print(row.scriptureContents, end = '\\n')\n",
    "        matches = findMatches(row.charSet, row.scriptureContents, 10)\n",
    "        # print(matches, end = \"\\n\\n\")\n",
    "\n",
    "        try:\n",
    "            if(len(matches[0]) > 15 and len(matches[1]) > 15):\n",
    "\n",
    "                row[\"wilfordMatch\"], row[\"scriptureMatch\"] = matches\n",
    "            else:\n",
    "                row[\"wilfordMatch\"] = row[\"scriptureMatch\"] = \"\"\n",
    "            \n",
    "        except:\n",
    "            row[\"wilfordMatch\"] = row[\"scriptureMatch\"] = \"\"\n",
    "            pass\n",
    "    # print(surrCharSetsTemp)\n",
    "\n",
    "    refMonsterTable = pd.concat([refMonsterTable, surrCharSetsTemp], ignore_index = True)\n",
    "\n",
    "    if passNum > 5:\n",
    "        break\n",
    "    else:\n",
    "        passNum = passNum + 1\n",
    "# print(refMonsterTable)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# refMonsterTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedMonsterTable = refMonsterTable[[\"ref\", \"wilfordMatch\", \"scriptureMatch\"]]\n",
    "\n",
    "trimmedMonsterTable = trimmedMonsterTable.query('scriptureMatch != \"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\derived_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LeRoy12358\\Documents\\School\\W23\\499 DS - Senior Project\\WWPapers\\scripts\\exploration.ipynb Cell 51\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LeRoy12358/Documents/School/W23/499%20DS%20-%20Senior%20Project/WWPapers/scripts/exploration.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trimmedMonsterTable\u001b[39m.\u001b[39;49mto_csv(\u001b[39m\"\u001b[39;49m\u001b[39m../derived_data/scriptureMatchData.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, index \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LeRoy12358/Documents/School/W23/499%20DS%20-%20Senior%20Project/WWPapers/scripts/exploration.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trimmedMonsterTable\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39m../../minions/data/scriptureMatchSpeechesData.csv\u001b[39m\u001b[39m\"\u001b[39m, index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\LeRoy12358\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LeRoy12358\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3721\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3710\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3712\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3713\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3714\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3718\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3719\u001b[0m )\n\u001b[1;32m-> 3721\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3722\u001b[0m     path_or_buf,\n\u001b[0;32m   3723\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3724\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3725\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3726\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3727\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3728\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3729\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3730\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3731\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3732\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3733\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3734\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3735\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3736\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3737\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3738\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LeRoy12358\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LeRoy12358\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\LeRoy12358\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\LeRoy12358\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:735\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 735\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    737\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    738\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    739\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LeRoy12358\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:598\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    596\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    597\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 598\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '..\\derived_data'"
     ]
    }
   ],
   "source": [
    "trimmedMonsterTable.to_csv(\"../derived_data/scriptureMatchData.csv\", index = False)\n",
    "trimmedMonsterTable.to_csv(\"../../minions/data/scriptureMatchSpeechesData.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedMonsterTable.to_csv(\"../derived_data/scriptureMatchData.csv\", index = False)\n",
    "trimmedMonsterTable.to_csv(\"../../minions/data/scriptureMatchSpeechesData.csv\", index = False)\n",
    "\n",
    "# trimmedMonsterTable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44839650a47afe4f123565aa95902dbfa099a4fa6c49cc733305d4612399f7fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
